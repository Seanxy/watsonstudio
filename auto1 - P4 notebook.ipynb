{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "################################################################################\n#Licensed Materials - Property of IBM\n#(C) Copyright IBM Corp. 2019\n#US Government Users Restricted Rights - Use, duplication disclosure restricted\n#by GSA ADP Schedule Contract with IBM Corp.\n################################################################################\n\nThe auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (\"License Terms\"), such agreements located in the link below.\nSpecifically, the Source Components and Sample Materials clause included in the License Information document for\nWatson Studio Auto-generated Notebook applies to the auto-generated notebooks. \nBy downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\nhttp://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## IBM AutoAI Auto-Generated Notebook v1.11.2\n### Representing Pipeline: P4 from run f8e6ab3f-1ec7-4b03-8085-2a122a771552\n\n**Note**: Notebook code generated using AutoAI will execute successfully.  If code is modified or reordered, there is no guarantee it will successfully execute, please read our documentation for more information https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:\nThe notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\nThe known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1. Set Up"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": "import sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import nan, dtype, mean\nimport autoai_libs\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nimport sklearn.ensemble\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2. Compose Pipeline"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\ndata_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\ndata_provenance = _input_metadata['data_provenance']\nif 'pos_label' in _input_metadata and learning_type == 'classification':\n    pos_label = _input_metadata['pos_label']\nelse:\n    pos_label = None\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": "import pandas as pd\ncsv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\ndf = None\nif type(data_provenance) is str: # try to load file specified\n    for encoding in csv_encodings:\n        try:\n            df = pd.read_csv(data_provenance, encoding=encoding) # your data file name here\n        except Exception as csv_exception:\n            print(csv_exception)\nif df is None: # try to load file from Cloud Object Storage\n    try:\n        data_location = data_provenance['input_data'][0]\n        print('data_location '+ str(data_location))\n        import boto3\n        session = boto3.session.Session()\n        cos = session.client(\n            service_name='s3',\n            aws_access_key_id=data_location['connection']['access_key_id'],\n            aws_secret_access_key=data_location['connection']['secret_access_key'],\n            endpoint_url=data_location['connection']['endpoint_url'],\n            verify=False\n        )\n        local_path = data_location['location']['path']\n        print('local_path ' + str(local_path))\n        cos.download_file(data_location['location']['bucket'],\n                     data_location['location']['path'],\n                     local_path)\n        for encoding in csv_encodings:\n            try:\n                df = pd.read_csv(local_path, encoding=encoding) # your data file name here\n            except Exception as csv_exception:\n                print(csv_exception)\n    except Exception as e:\n        print(e)\nif df is None:\n    raise(ValueError('Problem accessing or decoding csv data. May need csv_encoding string, or location or credential information to read dataframe from COS'))\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": "# Drop rows whose target is not defined\ntarget = target_label_name # your target name here\nif learning_type == 'regression':\n    df[target] = pd.to_numeric(df[target], errors='coerce')\ndf.dropna('rows', how='any', subset=[target], inplace=True)\ndf_y = df[target]\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_X = df.drop(columns=[target])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": "# split pipeline into preprocessing pipeline (which needs to see all data for e.g. known categories) and remainder\npreprocessor_index = -1\npreprocessing_steps = [] \nfor i, step in enumerate(pipeline.steps):\n    preprocessing_steps.append(step)\n    if step[0]=='preprocessor':\n        preprocessor_index = i\n        break\nif preprocessor_index >= 0:\n    preprocessing_pipeline = Pipeline(memory=pipeline.memory, steps=preprocessing_steps)\n    pipeline = Pipeline(steps=pipeline.steps[preprocessor_index+1:])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#delete CONTENTS of known_values_list before refitting, cloning or cross_validate-ing the pipeline, or previous values will be used.\nfor i in range(len(known_values_list)):\n    del known_values_list[0]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Preprocess X\n# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\npreprocessing_pipeline.fit(df_X.values)\nX_prep = preprocessing_pipeline.transform(df_X.values)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# determine learning_type and perform holdout split (stratify conditionally)\nif learning_type is None:\n    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n    from sklearn.utils.multiclass import type_of_target\n    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n        learning_type = 'classification'\n    else:\n        learning_type = 'regression'\n    print('learning_type determined by type_of_target as:',learning_type)\nelse:\n    print('learning_type specified as:',learning_type)\n    \nfrom sklearn.model_selection import train_test_split\nif learning_type == 'classification':\n    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# create a function to produce a scorer for a given positive label\ndef pos_label_scorer(scorer, pos_label):\n    kwargs = {'pos_label':pos_label}\n    for prop in ['needs_proba', 'needs_threshold']:\n        if prop+'=True' in scorer._factory_args():\n            kwargs[prop] = True\n    if scorer._sign == -1:\n        kwargs['greater_is_better'] = False\n    from sklearn.metrics import make_scorer\n    scorer=make_scorer(scorer._score_func, **kwargs)\n    return scorer"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# fit the remainder of the pipeline on the training data\npipeline.fit(X,y)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# predict on the holdout data\ny_pred = pipeline.predict(X_holdout)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# compute score for the optimization metric\n# scorer may need pos_label, but not all scorers take pos_label parameter\nfrom sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscore = None\n#score = scorer(pipeline, X_holdout, y_holdout)  # this would suffice for simple cases\npos_label = None  # if you want to supply the pos_label, specify it here\nif pos_label is None and 'pos_label' in _input_metadata:\n    pos_label=_input_metadata['pos_label']\ntry:\n    score = scorer(pipeline, X_holdout, y_holdout)\nexcept Exception as e1:\n    if pos_label is None or str(pos_label)=='':\n        print('You may have to provide a value for pos_label in order for a score to be calculated.')\n        raise(e1)\n    else:\n        exception_string=str(e1)\n        if 'pos_label' in exception_string:\n            try:\n                scorer = pos_label_scorer(scorer, pos_label=pos_label)\n                score = scorer(pipeline, X_holdout, y_holdout)\n                print('Retry was successful with pos_label supplied to scorer')\n            except Exception as e2:\n                print('Initial attempt to use scorer failed.  Exception was:')\n                print(e1)\n                print('')\n                print('Retry with pos_label failed.  Exception was:')\n                print(e2)\n        else:\n            raise(e1)\n\nif score is not None:\n    print(score)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# cross_validate pipeline using training data\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold, KFold\nif learning_type == 'classification':\n    fold_generator = StratifiedKFold(n_splits=cv_num_folds, random_state=random_state)\nelse:\n    fold_generator = KFold(n_splits=cv_num_folds, random_state=random_state)\ncv_results = cross_validate(pipeline, X, y, cv=fold_generator, scoring={optimization_metric:scorer})\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "cv_results"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}